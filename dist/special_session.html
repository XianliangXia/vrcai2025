<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>WorkShop - ACM SIGGRAPH VRCAI 2025</title>
    <link rel="stylesheet"
          href="https://fonts.googleapis.com/css2?family=Noto+Sans+SC:wght@300;400;500;700&display=swap">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css">
    <link rel="stylesheet" href="static/css/style.css">
</head>
<body>
<header>
    <div class="container header-container">
        <div class="logo">
            <div class="logo-text">VRCAI 2025</div>
        </div>
        <nav>
            <div class="nav-menu">
                <a href="index.html" class="nav-item">Home</a>
                <a href="call_for_papers.html" class="nav-item">Call for Papers</a>
                <a href="special_session.html" class="nav-item">WorkShop</a>
                <a href="submission.html" class="nav-item">Submission Guidelines</a>
                <a href="program.html" class="nav-item">Program</a>
                <a href="index.html" class="nav-item">
                    <del>Registration</del>
                </a>
                <a href="committee.html" class="nav-item">Committee</a>
                <a href="venue.html" class="nav-item">Venue</a>
                <a href="contact.html" class="nav-item">Contact Us</a>
            </div>
        </nav>
    </div>
</header>

<main>
    <div class="container">
        <section>
            <!--            <h1 class="section-title">Call for Papers <br> Session on Digital Heritage in the Age of AI and the Metaverse</h1>-->

            <section>
                <h2 class="section-title">Call for Papers (We have prepared three workshops) <br><br>
                    <hr style="height: 4px; background-color: black">
                    WorkShop-1:Session
                    on Digital Heritage in the Age of AI and the
                    Metaverse
                </h2>
                <div>
                    <div>
                        <img src="static/images/sessionpic.png" alt="sessionpic" width="90%">
                    </div>
                </div>
                <div>
                    <div style="margin-top: 5px">
                        <p>Digital heritage is undergoing a seismic transformation. The convergence of Artificial
                            Intelligence (AI),
                            immersive technologies (VR/AR/MR/XR), game engines, generative models, and interactive 3D
                            environments
                            has redefined how we capture, reconstruct, preserve, interpret, and engage with cultural
                            heritage. With
                            the rise of large language models (LLMs), multimodal AI, and the conceptual expansion of the
                            Metaverse,
                            we are witnessing not only technological evolution but also the reshaping of cultural
                            memory,
                            storytelling, and heritage interpretation.</p>
                        <p>This workshop invites contributions that explore these intersections from theoretical,
                            technical, and
                            applied perspectives. We welcome long papers (8 pages), short papers (4 pages) and
                            poster/demos (2
                            pages) that engage with critical issues, innovations, and applications in digital heritage
                            within the
                            context of these emerging technologies.</p>
                    </div>

                </div>
            </section>
            <section>
                <h2>Key Themes and Topics</h2>

                <p>We invite submissions on topics including but not limited to:</p>
                <!--1-->
                <div style="display: flex;">
                    <div style="flex: 1; box-sizing: border-box;">
                        <div class="topic-category">
                            <h3>AI and Generative Technologies in Heritage</h3>
                            <ul>
                                <li>Application of generative AI (text, image, 3D, audio, video) for heritage
                                    reconstruction and
                                    storytelling
                                </li>
                                <li>LLMs for historical knowledge retrieval, annotation, and conversational interfaces
                                </li>
                                <li>Ethical and epistemological challenges of generative heritage content</li>
                                <li>AI-enhanced knowledge graphs and ontologies for heritage contexts</li>
                            </ul>
                        </div>
                        <!--2-->
                        <div class="topic-category">
                            <h3>Immersive Technologies and the Metaverse</h3>
                            <ul>
                                <li>Heritage presence and embodiment in VR/AR/MR/XR environments</li>
                                <li>Digital twins of heritage sites and the role of persistent metaverse spaces</li>
                                <li>Multisensory interaction (haptics, soundscapes, spatial computing) for cultural
                                    immersion
                                </li>
                                <li>Co-presence, virtual gatherings, and digital rituals in heritage metaverse</li>
                                <li>Social VR, Social AR, Multiuser cultural environments</li>

                            </ul>
                        </div>
                        <!--3-->
                        <div class="topic-category">
                            <h3>Photogrammetry, Scanning, and 3D Reconstruction</h3>
                            <ul>
                                <li>High-resolution 3D capture and processing techniques</li>
                                <li>Advances in mesh optimization, PBR materials, and visual fidelity for heritage
                                    artifacts
                                </li>
                                <li>Integrating AI with photogrammetry for semantic labeling and restoration</li>

                            </ul>
                        </div>
                        <!--4-->
                        <div class="topic-category">
                            <h3>Game Engines and Real-Time Simulation</h3>
                            <ul>
                                <li>Use of Unity, Unreal Engine, and other platforms for cultural visualization and
                                    narrative
                                </li>
                                <li>Procedural generation of heritage environments</li>
                                <li>Simulation of intangible heritage (e.g., dance, ritual, craft, language) in
                                    real-time
                                    engines
                                </li>
                                <li>AI-driven NPCs and conversational agents for interactive storytelling</li>

                            </ul>
                        </div>
                        <!--    5-->
                        <div class="topic-category">
                            <h3>Narrative, Storytelling, and Interpretive Design</h3>
                            <ul>
                                <li>Computational storytelling in heritage environments</li>
                                <li>Interactive and adaptive narratives using LLMs and multimodal AI</li>
                                <li>Placemaking, temporality, and memory in digital storytelling</li>
                                <li>Cross-cultural, decolonial, and inclusive narrative frameworks</li>

                            </ul>
                        </div>
                        <!--    6-->
                        <div class="topic-category">
                            <h3>Preservation, Access, and Archival Futures</h3>
                            <ul>
                                <li>AI-assisted curation, metadata enrichment, and archival automation</li>
                                <li>Designing heritage platforms for long-term digital preservation</li>
                                <li>Blockchain, NFTs, and ownership in cultural heritage digitization</li>
                                <li>Policy, provenance, and cultural sensitivity in AI-generated heritage content</li>


                            </ul>
                        </div>
                    </div>
                    <div style="flex: 1; box-sizing: border-box;">
                        <!--  7  -->
                        <div class="topic-category">
                            <h3>Human-Computer Interaction and User Experience</h3>
                            <ul>
                                <li>Measuring presence, engagement, and learning in immersive heritage applications</li>
                                <li>Designing for diverse user groups (age, ability, cultural background)</li>
                                <li>Embodied cognition, affective computing, and situated experience in heritage XR</li>

                            </ul>
                        </div>
                        <!--    8-->
                        <div class="topic-category">
                            <h3>Speculative Futures and Critical Perspectives</h3>
                            <ul>
                                <li>Posthuman heritage and AI as co-authors of history</li>
                                <li>Heritage in the age of synthetic realities and deepfakes</li>
                                <li>Technological obsolescence and sustainability of immersive heritage systems</li>
                                <li>Bias, authenticity, and historiographical issues in AI/Metaverse interpretations
                                </li>


                            </ul>
                        </div>
                        <!--    9-->
                        <div class="topic-category">
                            <h3>Crowdsourcing, Citizen Science, and Participatory Heritage</h3>
                            <ul>
                                <li>AI-supported platforms for community co-curation of heritage content</li>
                                <li>Crowdsourced annotation and validation using gamified interfaces</li>
                                <li>Participatory scanning and storytelling using mobile XR</li>
                                <li>Ethical frameworks for community-led digital heritage initiatives</li>
                            </ul>
                        </div>
                        <!--    10-->
                        <div class="topic-category">
                            <h3>Security, Trust, and Governance in Digital Heritage</h3>
                            <ul>
                                <li>Cybersecurity and data integrity in heritage metaverse platforms</li>
                                <li>AI for detecting misinformation and falsified heritage representations</li>
                                <li>Governance models for open-access heritage data</li>
                                <li>Cultural IP rights, repatriation, and digital sovereignty in AI environments</li>

                            </ul>
                        </div>
                        <!--    11-->
                        <div class="topic-category">
                            <h3>Embodied AI and Performative Heritage</h3>
                            <ul>
                                <li>Robots and AI agents as performers of intangible heritage (e.g., dance, ritual,
                                    ceremony)
                                </li>
                                <li>Human-AI co-performance in cultural heritage contexts</li>
                                <li>Training embodied agents with cultural etiquette and performative norms</li>
                                <li>Responsive environments and affective feedback loops in heritage installations</li>


                            </ul>
                        </div>
                    </div>
                </div>

            </section>
            <h2>Topic Session Invited by Industry</h2>

            <p>NVIDIA added a topic session (1 hour) which will run immediately after the Session on Digital Heritage.
                It will be special presentations + Q&A. This is a great opportunity to communicate with the cutting-edge
                of the industry</p>

            <div class="info-item">
                <div class="info-icon"><i class="fas fa-user-graduate"></i></div>
                <h3>Co-host: Dr. Charles Cheung</h3>
                <p>Deputy Director, NVIDIA AI Technology Center Hong Kong, NVIDIA Corporation</p>
            </div>
            <div class="info-item">
                <div class="info-icon"><i class="fas fa-user-graduate"></i></div>
                <h3>Co-host: Cliff Ho</h3>
                <p>Senior Solution Architect & Omniverse Lead, NVIDIA AI Technology Center Hong Kong, NVIDIA
                    Corporation</p>
            </div>

            <h2>Session Chairs</h2>
                 <p>
                Dr. Yuhan Ji, Assistant Professor, School of Culture and Creativity, Beijing Normal–Hong Kong Baptist University.
            </p>
            
            <p>
                Prof. Eugene Ch'ng, Dean, School of Culture and Creativity, Beijing Normal–Hong Kong Baptist University/
                Director, Centre for Computational Culture and Heritage | NVIDIA Deep Learning Institute/
                Editor-in-Chief, Presence: Virtual and Augmented Reality, MIT Press.
            </p>
       
            <p>
                Prof. Keng Leng Siau, Chair Professor, School of Computing and Information Systems, Singapore Management University.
            </p>
        </section>


        <hr style="height: 4px; background-color: black">
        <section>

            <section>
                <h2 class="section-title"><br> WorkShop-2: Session on Video Understanding and Grounding (VUG) in the Age
                    of AI and the Metaverse</h2>

                <div>
                    <div style="margin-top: 5px">
                        <p>This workshop aims to advance the frontier of vision-language understanding in video content,
                            focusing on the critical intersection of visual perception and natural language processing
                            in temporal media. Video grounding represents a fundamental challenge in multimodal AI,
                            requiring models to precisely localize and understand video segments based on natural
                            language descriptions, bridging the semantic gap between human language and visual temporal
                            dynamics.</p>
                        <p>With the exponential growth of video content across platforms and the increasing demand for
                            intelligent video analysis, the field of video grounding has emerged as a crucial research
                            area. This encompasses various tasks including temporal video grounding, spatial-temporal
                            localization, video question answering, video captioning, and cross-modal video retrieval.
                            Recent advances in large vision-language models such as VideoBERT, Video-ChatGPT, Valley,
                            and LLaVA-Video have demonstrated remarkable capabilities in understanding video content
                            through natural language interfaces.</p>
                        <p>The workshop will explore cutting-edge developments in self-supervised vision and language
                            pre-training for video understanding, addressing the unique challenges posed by temporal
                            dynamics, motion patterns, and long-range dependencies in video sequences. We will examine
                            novel architectures that effectively integrate visual features across time with linguistic
                            representations, enabling more robust and accurate video grounding capabilities.</p>
                        <p>This workshop will provide a comprehensive platform for researchers, practitioners, and
                            industry professionals to explore the latest methodologies, datasets, and applications in
                            video grounding and vision-language video understanding. We welcome contributions that push
                            the boundaries of current capabilities and address real-world challenges in video content
                            analysis.</p>
                    </div>

                </div>
            </section>
            <section>
                <h2>Key Themes and Topics</h2>

                <p>We invite submissions on topics including but not limited to:</p>
                <!--1-->
                <div style="display: flex;">
                    <div style="flex: 1; box-sizing: border-box;">
                        <div class="topic-category">
                            <h3>Temporal Video Grounding</h3>
                            <ul>
                                <li>Precise localization of video segments based on natural language queries</li>
                            </ul>
                        </div>
                        <!--2-->
                        <div class="topic-category">
                            <h3>Self-supervised Vision and Language Pre-training</h3>
                            <ul>
                                <li>Novel pre-training strategies for video-language understanding without extensive
                                    manual annotations
                                </li>

                            </ul>
                        </div>
                        <!--3-->
                        <div class="topic-category">
                            <h3>Real-world Video Understanding Tasks</h3>
                            <ul>
                                <li>New datasets and benchmarks that reflect practical applications in video analysis
                                    and retrieval
                                </li>

                            </ul>
                        </div>
                        <!--4-->
                        <div class="topic-category">
                            <h3>Text-to-Video Generation and Editing</h3>
                            <ul>
                                <li>Advanced techniques for generating and manipulating video content through textual
                                    instructions
                                </li>

                            </ul>
                        </div>
                        <!--    5-->
                        <div class="topic-category">
                            <h3>External Knowledge Integration</h3>
                            <ul>
                                <li>Incorporating world knowledge and common sense reasoning into video understanding
                                    systems
                                </li>

                            </ul>
                        </div>
                        <!--    6-->
                        <div class="topic-category">
                            <h3>Visually-grounded Video Captioning</h3>
                            <ul>
                                <li>Generating detailed and contextually accurate descriptions of video content</li>


                            </ul>
                        </div>
                        <!--  7  -->
                        <div class="topic-category">
                            <h3>Language-grounded Video Recognition</h3>
                            <ul>
                                <li>Using natural language to guide video classification, detection, and segmentation
                                    tasks
                                </li>

                            </ul>
                        </div>
                    </div>
                    <div style="flex: 1; box-sizing: border-box;">

                        <!--    8-->
                        <div class="topic-category">
                            <h3>Embodied Video Understanding</h3>
                            <ul>
                                <li>Applications in robotics and autonomous systems requiring video-language
                                    comprehension
                                </li>


                            </ul>
                        </div>
                        <!--    9-->
                        <div class="topic-category">
                            <h3>Multilingual Video Grounding</h3>
                            <ul>
                                <li>Cross-lingual and multilingual approaches to video understanding and localization
                                </li>
                            </ul>
                        </div>
                        <!--    10-->
                        <div class="topic-category">
                            <h3>Robustness and Limitations</h3>
                            <ul>
                                <li>Addressing shortcomings of existing large vision-language models in video
                                    understanding tasks
                                </li>

                            </ul>
                        </div>
                        <!--    11-->
                        <div class="topic-category">
                            <h3>Ethics and Bias</h3>
                            <ul>
                                <li>Examining fairness, privacy, and ethical considerations in video grounding systems
                                </li>


                            </ul>
                        </div>

                        <!--    12-->
                        <div class="topic-category">
                            <h3>Interdisciplinary Approaches</h3>
                            <ul>
                                <li>Incorporating insights from cognitive science, linguistics, and human perception
                                    studies
                                </li>


                            </ul>
                        </div>

                        <!--    13-->
                        <div class="topic-category">
                            <h3>Explainability and Interpretability</h3>
                            <ul>
                                <li>Developing transparent and interpretable video grounding models</li>


                            </ul>
                        </div>

                    </div>
                </div>

                <h2>Workshop Objectives</h2>

                <p>The workshop aims to foster collaboration between researchers working on different aspects of video
                    grounding and vision-language understanding. We seek to identify emerging trends, discuss technical
                    challenges, and explore potential solutions for scaling video understanding to real-world
                    applications. The workshop will also serve as a forum for discussing the societal implications of
                    advanced video understanding technologies and their responsible deployment.</p>
                <p>We encourage submissions that not only advance the technical state-of-the-art but also consider
                    practical deployment scenarios, computational efficiency, and the broader impact of video grounding
                    technologies on society. The workshop welcomes both theoretical contributions and empirical studies
                    that demonstrate novel applications of vision-language models in video understanding tasks.</p>

                <h2>Session Chairs</h2>
                <p>
                    Prof. Jun Wan, Institute of Automation, Chinese Academy of Sciences.
                </p>
                <p>
                    Dr. Tongbao Chen, Lecturer, School of Computer Science, Guangdong Polytechnic Normal University.
                </p>

            </section>
        </section>

<!--            workshop3-->
            <hr style="height: 4px; background-color: black">
            <section>
                <h2 class="section-title"><br> WorkShop-3: Session on AI-Powered Medical Image Analysis and Computer
                    Graphics in Healthcare</h2>

                <div>
                    <div style="margin-top: 5px">
                        <p>This workshop explores the cutting-edge intersection of medical image processing and computer
                            graphics, aiming to foster innovation and collaboration between clinicians, scientists, and
                            engineers. We will delve into how advanced computational techniques are revolutionizing the
                            way we acquire, analyze, and visualize medical data. The session will cover key topics such
                            as the segmentation of anatomical structures from MRI/CT scans, 3D reconstruction of organs
                            and tissues, the development of interactive surgical planning systems, and the creation of
                            immersive educational tools using augmented and virtual reality (AR/VR). Participants will
                            gain insights into how graphics rendering pipelines enhance the interpretation of complex
                            medical datasets, transforming them into intuitive, high-fidelity visual models. The
                            workshop will feature presentations on latest research, hands-on demonstrations of
                            state-of-the-art software tools, and discussions on future trends and challenges in creating
                            clinically actionable visualizations.
                        </p>
                    </div>

                </div>
            </section>
            <section>
                <h2>Key Themes and Topics</h2>

                <p>We invite submissions on topics including but not limited to:</p>
                <!--1-->
                <div style="display: flex;">
                    <div style="flex: 1; box-sizing: border-box;">
                        <div class="topic-category">
                            <h3>Medical Image Segmentation</h3>
                            <ul>
                                <li>Automated segmentation of organs, tumors, and anatomical structures using deep
                                    learning
                                </li>
                                <li>Uncertainty quantification in medical image segmentation</li>
                            </ul>
                        </div>

                        <div class="topic-category">
                            <h3>Computer-Aided Diagnosis and Detection</h3>
                            <ul>
                                <li>AI-powered diagnostic systems for cancer detection, cardiovascular diseases, and
                                    neurological disorders
                                </li>
                                <li>Early disease detection and screening applications using computer vision</li>
                            </ul>
                        </div>

                        <div class="topic-category">
                            <h3>Multi-Modal Medical Image Analysis</h3>
                            <ul>
                                <li>Integration of CT, MRI, PET, ultrasound, and digital pathology data</li>
                                <li>Cross-modal learning and fusion techniques for comprehensive diagnosis</li>
                            </ul>
                        </div>

                        <div class="topic-category">
                            <h3>Deep Learning Architectures for Medical Imaging</h3>
                            <ul>
                                <li>Convolutional Neural Networks (CNNs) and their medical applications</li>
                                <li>Vision Transformers (ViTs) and attention mechanisms in medical image analysis</li>
                                <li>Generative models for medical image synthesis and augmentation</li>
                            </ul>
                        </div>

                        <div class="topic-category">
                            <h3>Treatment Planning and Surgical Guidance</h3>
                            <ul>
                                <li>AI-assisted radiotherapy planning and dose optimization</li>
                                <li>Image-guided surgery and real-time intraoperative assistance</li>
                                <li>Personalized treatment planning based on medical imaging analysis</li>
                            </ul>
                        </div>

                        <div class="topic-category">
                            <h3>Federated Learning and Privacy-Preserving Methods</h3>
                            <ul>
                                <li>Collaborative learning across medical institutions while preserving patient
                                    privacy
                                </li>
                                <li>Distributed training of medical AI models without centralized data sharing</li>
                            </ul>
                        </div>
                    </div>
                    <div style="flex: 1; box-sizing: border-box;">
                        <div class="topic-category">
                            <h3>Interpretability and Explainable AI</h3>
                            <ul>
                                <li>Developing transparent and interpretable medical image analysis models</li>
                                <li>Visualization techniques for understanding AI decision-making in clinical contexts
                                </li>
                            </ul>
                        </div>

                        <div class="topic-category">
                            <h3>Clinical Deployment and Validation</h3>
                            <ul>
                                <li>Real-world implementation challenges and solutions for medical AI systems</li>
                                <li>Regulatory compliance and clinical validation of AI-powered diagnostic tools</li>
                            </ul>
                        </div>

                        <div class="topic-category">
                            <h3>Robustness and Generalization</h3>
                            <ul>
                                <li>Addressing domain shift and generalization across different medical institutions
                                </li>
                                <li>Handling noisy, incomplete, or low-quality medical images</li>
                            </ul>
                        </div>

                        <div class="topic-category">
                            <h3>Emerging Technologies and Future Directions</h3>
                            <ul>
                                <li>Wearable imaging devices and point-of-care diagnostics</li>
                                <li>Virtual and augmented reality applications in medical imaging</li>
                                <li>Integration of AI with advanced imaging modalities</li>
                            </ul>
                        </div>

                        <div class="topic-category">
                            <h3>Ethics and Bias in Medical AI</h3>
                            <ul>
                                <li>Addressing algorithmic bias and ensuring fairness in medical image analysis</li>
                                <li>Ethical considerations in AI-powered healthcare decision-making</li>
                            </ul>
                        </div>

                        <div class="topic-category">
                            <h3>Dataset Development and Benchmarking</h3>
                            <ul>
                                <li>Creation of large-scale, annotated medical imaging datasets</li>
                                <li>Standardized evaluation metrics and benchmarks for medical image analysis</li>
                            </ul>
                        </div>

                    </div>
                </div>
  <h2>Session Chairs</h2>
                <p>
                    Prof. Yang Wen, School of Electronic and Information Engineering, Shenzhen University.
                </p>
                <p>
                    Prof. Jie Zhang, Faculty of Applied Sciences, Macao Polytechnic University, Macau, China. E-mail: jpeter.zhang@mpu.edu.mo
                </p>
                <p>
                    Prof. Yang Wen, College of Electronics and Information Engineering, Shenzhen University, Shenzhen, China. E-mail: wen_yang@szu.edu.cn
                </p>
            </section>
        
        <hr style="height: 4px; background-color: black">
        <section>
            <h2 class="section-title">Submission Requirements</h2>

            <p>All submissions must follow the ACM SIGGRAPH paper format and be written in English.</p>
            <div class="paper-types">
                <div class="paper-type">
                    <h3>Full Papers</h3>
                    <div class="paper-type-meta">
                        <span><i class="fas fa-file-alt"></i> 8 pages (including references)</span>
                        <span><i class="fas fa-database"></i> Indexed in ACM Digital Library</span>
                        <span><i class="fas fa-search"></i> EI Compendex indexed</span>
                    </div>
                    <p>Full papers should contain original research contributions with detailed method descriptions,
                        experimental validation, and thorough discussion.</p>
                </div>

                <div class="paper-type">
                    <h3>Short Papers</h3>
                    <div class="paper-type-meta">
                        <span><i class="fas fa-file-alt"></i> 4 pages (including references)</span>
                        <span><i class="fas fa-database"></i> Indexed in ACM Digital Library</span>
                        <span><i class="fas fa-search"></i> EI Compendex indexed</span>
                    </div>
                    <p>Short papers may cover ongoing research work, preliminary research results, or technical
                        introductions.</p>
                </div>

                <div class="paper-type">
                    <h3>Posters/Demos</h3>
                    <div class="paper-type-meta">
                        <span><i class="fas fa-file-alt"></i> 2 pages (including references)</span>
                        <span><i class="fas fa-database"></i> Indexed in ACM Digital Library</span>
                    </div>
                    <p>Poster and demo abstracts should concisely describe the research content and innovation, suitable
                        for showcasing system prototypes or interactive applications.</p>
                </div>
            </div>
            <div class="info-item">
                <div class="info-icon"><i class="fas fa-file-pdf"></i></div>
                <h3>Paper Template</h3>
                <p>Please use the <a href="https://www.acm.org/publications/proceedings-template" target="_blank">ACM
                    template</a> to format your paper</p>
            </div>

            <p class="mt-3">Submission process:</p>
            <ol>
                <li>Prepare your paper in PDF format according to requirements</li>
                <li>Visit our online submission system to submit your work</li>
                <li>Submit author information and paper abstract</li>
                <li>Upload your PDF file</li>
            </ol>

            <div class="text-center mt-3">
                <a href="submission.html" class="btn btn-primary">Online Submission System <i
                        class="fas fa-arrow-right"></i></a>
            </div>
        </section>

        <section>
            <h2 class="section-title">Paper Review</h2>

            <p>All submissions will undergo a rigorous double-blind review process. Please remove any information that
                could reveal the authors' identities from your paper.</p>

            <div class="info-item">
                <div class="info-icon"><i class="fas fa-award"></i></div>
                <h3>Paper Awards</h3>
                <p>The conference will present Best Paper Awards and Best Presentation Awards to recognize outstanding
                    research contributions.</p>
            </div>
        </section>

        <section>
            <h2 class="section-title">Publication</h2>

            <p>All accepted papers will be published in the ACM SIGGRAPH VRCAI 2025 Conference Proceedings and included
                in the ACM Digital Library (EI Compendex indexed).</p>

            <p>Outstanding papers will have the opportunity to be recommended to the following partner journals:</p>
            <ul>
                <li>Computational Visual Media (Springer)</li>
                <li>Computer Animation and Virtual Worlds (Wiley)</li>
                <li>Virtual Reality & Intelligent Hardware (Elsevier)</li>
                <li>Visual Computing for Industry, Biomedicine, and Art (Springer)</li>
            </ul>
        </section>

        <section class="cta-section">
            <h2 class="cta-title">Submit Now and Share Your Research</h2>
            <p>We welcome diverse perspectives from academia, cultural institutions, creative industries, and
                independent practitioners. This is an opportunity to define the next frontier of digital heritage
                practice and scholarship. Let us shape the future of cultural memory—together.</p>
            <div class="cta-buttons">
                <a href="submission.html" class="btn-white">Start Submission</a>
                <a href="dates.html" class="btn-outline-white">Check Deadlines</a>
            </div>
        </section>
    </div>
</main>

<footer>
    <div class="container">
        <div class="footer-content">
            <div class="footer-column">
                <h3>VRCAI 2025</h3>
                <ul class="footer-links">
                    <li><a href="index.html">About</a></li>
                    <li><a href="program.html">Program</a></li>
                    <li><a href="index.html">Registration</a></li>
                    <li><a href="venue.html">Venue</a></li>
                    <li><a href="committee.html">Committee</a></li>
                </ul>
            </div>
            <div class="footer-column">
                <h3>Submissions</h3>
                <ul class="footer-links">
                    <li><a href="call_for_papers.html">Call for Papers</a></li>
                    <li><a href="special_session.html">WorkShop</a></li>
                    <li><a href="submission.html">Paper Submission</a></li>
                    <li><a href="dates.html">Important Dates</a></li>

                </ul>
            </div>
            <div class="footer-column">
                <h3>Contact Us</h3>
                <ul class="footer-contact">
                    <li><i class="fas fa-envelope"></i> wxwang@must.edu.mo</li>

                    <li><i class="fas fa-map-marker-alt"></i> Macau University of Science and Technology, Avenida Wai
                        Long, Taipa, Macau, China
                    </li>
                </ul>
            </div>
        </div>
        <div class="footer-bottom">
            <p>&copy; 2025 ACM SIGGRAPH VRCAI. All rights reserved.</p>
        </div>
    </div>
</footer>

<a href="#" class="back-to-top"><i class="fas fa-arrow-up"></i></a>

<script src="static/js/main.js"></script>
</body>
</html>

